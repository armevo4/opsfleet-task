# Airflow on EKS Deployment Guide

## Research and Solution Choice

After thorough research on the best practices for deploying Airflow on Amazon EKS, I found that the most suitable and robust solution is provided by the AWS Labs Airflow on EKS repository. This solution is widely recognized for its seamless integration with AWS services and Kubernetes, allowing for a highly scalable and manageable Airflow setup.


To test the setup and demonstrate its functionality, I cloned the repository and made the necessary modifications to enable public access. This allows for easy testing of the Airflow web UI and DAG executions in a development environment, while also highlighting the flexibility of the solution. For production, more secure configurations, such as internal load balancers and restricted access through VPC, can be applied as needed.

## Architecture Decisions
### 1. Executor Choice: Kubernetes Executor

We have chosen the Kubernetes Executor for Airflow due to several key advantages that it provides when running on Amazon EKS:

. Dynamic scaling: Each task is executed in a separate pod, allowing for automatic and dynamic scaling of resources. This makes it easy to handle varying workloads efficiently.

. Resource isolation: Every task runs in its own isolated pod, ensuring that failures in one task do not affect others. This provides a high degree of reliability.

. Cost-effectiveness: Resources are only consumed when tasks are running, helping to minimize costs, especially for intermittent workloads.

. Native Kubernetes integration: Since we are using EKS, the Kubernetes Executor integrates natively, leveraging our existing Kubernetes infrastructure without requiring any additional overhead for orchestration.

The Kubernetes Executor provides a balance between scalability, cost, and isolation, making it the most suitable choice for our workload on Amazon EKS.


### 2. Logs from Workers in the Airflow Console

One of the essential requirements was to ensure that logs generated by worker pods (which handle individual tasks) should be visible directly in the Airflow console. This is critical for monitoring and debugging DAG executions.


To meet this requirement, we have configured Airflow to use Amazon S3 as the remote storage backend for logs. Worker logs are automatically shipped to an S3 bucket, which is configured to be accessible via the Airflow web UI. This ensures that:

. Logs are centralized in one location (S3), making them easier to manage and archive.

. They can be accessed and reviewed through the Airflow UI, making it convenient for developers and operators to monitor task execution in real time.

The S3 integration not only helps in log visibility but also scales with the increasing volume of logs generated by tasks.


### 3. DAG Deployment Method: Git-Sync
For deploying DAGs to the Airflow cluster, we decided to use the Git-Sync sidecar container approach. This method offers several advantages:


. Version control: Since DAGs are stored in a Git repository, we can manage changes and versions efficiently. Any changes to the DAGs can be tracked, and previous versions can be restored easily.

. Easy rollback capability: In case of errors or issues with new DAGs, it is easy to revert to a previous stable version, reducing downtime and troubleshooting complexity.

. Familiar workflow for developers: Developers are already familiar with Git-based workflows, so adopting this approach minimizes the learning curve. They can continue using their existing Git workflows to create and update DAGs.

. No need to rebuild containers: Unlike some other methods, Git-Sync doesn't require rebuilding the Airflow image when DAGs change. This results in faster updates and more flexibility in managing DAGs.

. Automated synchronization: Git-Sync runs as a sidecar container, continuously syncing the DAGs from the repository to the Airflow pods. This ensures that the latest DAGs are always available in the cluster without manual intervention.

We chose this approach due to its simplicity, flexibility, and alignment with modern DevOps practices. By using Git-Sync, we ensure that DAG deployment is streamlined, versioned, and reliable.


### Deploying the Solution

Clone the repository

```
 git clone https://github.com/armevo4/opsfleet-task.git


```

Navigate into self-managed-airflow directory and run install.sh script

```
cd airplow-task/self-managed-airflow
chmod +x install.sh
./install.sh
```


### Verify the resources
#### Create kubectl config
Update the placeholder for AWS region and run the below command.

```
mv ~/.kube/config ~/.kube/config.bk
aws eks update-kubeconfig --region <region>  --name self-managed-airflow
```

### Describe the EKS Cluster

```
aws eks describe-cluster --name self-managed-airflow


```

### Verify the EFS PV and PVC created by this deployment

```
kubectl get pvc -n airflow

NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
airflow-dags   Bound    pvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            efs-sc         73m

kubectl get pv -n airflow
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE
pvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            Delete           Bound    airflow/airflow-dags           efs-sc                  74m

```

### Verify the EFS Filesystem

```
aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text
```

### Verify S3 bucket created for Airflow logs

```
aws s3 ls | grep airflow-logs-


```

### Verify the Airflow deployment

```
kubectl get deployment -n airflow

NAME                READY   UP-TO-DATE   AVAILABLE   AGE
airflow-pgbouncer   1/1     1            1           77m
airflow-scheduler   2/2     2            2           77m
airflow-statsd      1/1     1            1           77m
airflow-triggerer   1/1     1            1           77m
airflow-webserver   2/2     2            2           77m

```

### Fetch Postgres RDS password
Amazon Postgres RDS database password can be fetched from the Secrets manager

. Login to AWS console and open secrets manager
. Click on postgres secret name
. Click on Retrieve secret value button to verify the Postgres DB master password

### Login to Airflow Web UI
This deployment creates an Ingress object with public LoadBalancer(internal # Private Load Balancer can only be accessed within the VPC) for demo purpose For production workloads, you can modify airflow-values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port.

Execute the following command to get the ALB DNS name

```
kubectl get ingress -n airflow

NAME                      CLASS   HOSTS   ADDRESS                                                                PORTS   AGE
airflow-airflow-ingress   alb     *       k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com   80      88m

```

The above ALB URL will be different for you deployment. So use your URL and open it in a browser

By default, Airflow creates a default user with admin and password as admin

Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user

### Execute Sample Airflow Job
. Login to Airflow WebUI

. Click on DAGs link on the top of the page. This will show dags pre-created by the GitSync feature

. Execute the hello_world_scheduled_dag DAG by clicking on Play button (>)

. Verify the DAG execution from Graph link

. All the Tasks will go green after few minutes

. Click on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3



### How to Set Up Git-Sync:
#### 1. Create a Git Repository for DAGs

. Begin by creating a dedicated Git repository that will contain your DAG files. This repository will act as the source of truth for your DAGs.

. Initialize a new Git repository (if one doesnâ€™t already exist) and add your DAGs to the repository:

```
git init
git add dags/  # assuming you have a dags/ directory
git commit -m "Initial commit of DAGs"
git remote add origin <your-git-repo-url>
git push origin master

```

#### 2. Configure Git-Sync in Airflow

. Modify the Airflow deployment to include a Git-Sync sidecar container in the airflow-webserver and airflow-scheduler pods. This container will clone and keep the DAGs synchronized from the Git repository to the local file system where Airflow can access them.

. Update the Airflow Helm values file or Kubernetes manifests to include the following Git-Sync configuration.

#### 3. Create Kubernetes Secret for Git Credentials

. To securely provide Git credentials to Git-Sync, create a Kubernetes secret. This secret will store your Git username and password/token used for authentication.

#### 4. Mount DAGs into Airflow

. Ensure that the DAGs synchronized by Git-Sync are accessible by the Airflow webserver and scheduler. You need to mount the same volume in the Airflow containers where the DAGs will be stored. In your Airflow deployment file, update the volume mounts.

#### 5. Test DAG Synchronization

. Once the Git-Sync sidecar is up and running, any changes you push to the Git repository should automatically appear in the /opt/airflow/dags directory within your Airflow pods.

. You can verify that the DAGs are synchronized by checking the contents of the dags directory inside the Airflow container.

```
kubectl exec -n airflow <airflow-webserver-pod> -- ls /opt/airflow/dags

```

. To test the workflow:

    . Push a new DAG to the Git repository.

    . Wait for Git-Sync to pull the new changes (within the configured polling interval).

    . Confirm the new DAG is listed in the Airflow UI.

#### 6. Monitor DAG Updates

. Git-Sync will continuously check for changes in the repository (based on the polling interval). Whenever a new commit is pushed to the main branch of your DAG repository, Git-Sync will fetch the updates, and Airflow will automatically detect the new DAGs.


. You can also monitor the Git-Sync logs to ensure synchronization is working correctly:

```
kubectl logs <airflow-webserver-pod> -c git-sync

```

