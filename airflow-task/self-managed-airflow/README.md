# Airflow on EKS Deployment Guide

## Research and Solution Choice

After thorough research on the best practices for deploying Airflow on Amazon EKS, I found that the most suitable and robust solution is provided by the AWS Labs Airflow on EKS repository. This solution is widely recognized for its seamless integration with AWS services and Kubernetes, allowing for a highly scalable and manageable Airflow setup.


To test the setup and demonstrate its functionality, I cloned the repository and made the necessary modifications to enable public access. This allows for easy testing of the Airflow web UI and DAG executions in a development environment, while also highlighting the flexibility of the solution. For production, more secure configurations, such as internal load balancers and restricted access through VPC, can be applied as needed.

## Architecture Decisions
### 1. Executor Choice: Kubernetes Executor

We have chosen the Kubernetes Executor for Airflow due to several key advantages that it provides when running on Amazon EKS:

. Dynamic scaling: Each task is executed in a separate pod, allowing for automatic and dynamic scaling of resources. This makes it easy to handle varying workloads efficiently.

. Resource isolation: Every task runs in its own isolated pod, ensuring that failures in one task do not affect others. This provides a high degree of reliability.

. Cost-effectiveness: Resources are only consumed when tasks are running, helping to minimize costs, especially for intermittent workloads.

. Native Kubernetes integration: Since we are using EKS, the Kubernetes Executor integrates natively, leveraging our existing Kubernetes infrastructure without requiring any additional overhead for orchestration.

The Kubernetes Executor provides a balance between scalability, cost, and isolation, making it the most suitable choice for our workload on Amazon EKS.


### 2. Logs from Workers in the Airflow Console

One of the essential requirements was to ensure that logs generated by worker pods (which handle individual tasks) should be visible directly in the Airflow console. This is critical for monitoring and debugging DAG executions.


To meet this requirement, we have configured Airflow to use Amazon S3 as the remote storage backend for logs. Worker logs are automatically shipped to an S3 bucket, which is configured to be accessible via the Airflow web UI. This ensures that:

. Logs are centralized in one location (S3), making them easier to manage and archive.

. They can be accessed and reviewed through the Airflow UI, making it convenient for developers and operators to monitor task execution in real time.

The S3 integration not only helps in log visibility but also scales with the increasing volume of logs generated by tasks.


### 3. DAG Deployment Method: Git-Sync
For deploying DAGs to the Airflow cluster, we decided to use the Git-Sync sidecar container approach. This method offers several advantages:


. Version control: Since DAGs are stored in a Git repository, we can manage changes and versions efficiently. Any changes to the DAGs can be tracked, and previous versions can be restored easily.

. Easy rollback capability: In case of errors or issues with new DAGs, it is easy to revert to a previous stable version, reducing downtime and troubleshooting complexity.

. Familiar workflow for developers: Developers are already familiar with Git-based workflows, so adopting this approach minimizes the learning curve. They can continue using their existing Git workflows to create and update DAGs.

. No need to rebuild containers: Unlike some other methods, Git-Sync doesn't require rebuilding the Airflow image when DAGs change. This results in faster updates and more flexibility in managing DAGs.

. Automated synchronization: Git-Sync runs as a sidecar container, continuously syncing the DAGs from the repository to the Airflow pods. This ensures that the latest DAGs are always available in the cluster without manual intervention.

We chose this approach due to its simplicity, flexibility, and alignment with modern DevOps practices. By using Git-Sync, we ensure that DAG deployment is streamlined, versioned, and reliable.



#### 1.the Second options is 

#### Deploying DAGs via GitOps with Argo CD
##### Overview

GitOps is a methodology that relies on Git repositories to manage the entire system's state. Using Argo CD, a GitOps tool, we will synchronize the DAGs in a Git repository with the Airflow cluster running on Kubernetes. Argo CD continuously monitors the Git repository for changes and automatically deploys those changes to the Airflow cluster.


This method was chosen because:

. Automation: GitOps provides an automated way to deploy DAGs, reducing the chances of manual errors.

. Version Control: Every change to a DAG is tracked in Git, enabling easy rollback and audit of changes.

. Continuous Deployment: Argo CD continuously watches for new DAGs or updates to existing DAGs, automatically applying those changes to the Airflow environment without manual intervention.

. Kubernetes-native: Since Airflow is running on an EKS cluster (as per our setup), using Argo CD integrates well with the Kubernetes environment, ensuring consistency between Git and the live environment.


#### How It Works
Git Repository Setup:

. You will create a Git repository to store all the DAG files (Python scripts) and configuration files for the Airflow cluster.

. Every time a new DAG is created or an existing DAG is updated, you push the changes to this repository.

Argo CD Setup:

. Argo CD is installed on your Kubernetes cluster where Airflow is running.

. A Helm chart or Kustomize configuration is created in the Git repository to describe how the DAGs are deployed to the Airflow cluster.

. The dags/ directory inside the repository contains all the DAG files.


Continuous Deployment:

. Argo CD is configured to watch the Git repository.

. Whenever a new DAG is added or an existing DAG is modified and pushed to the repository, Argo CD detects the change.

. Argo CD synchronizes the dags/ directory from the Git repository with the Airflow cluster's DAG directory (mounted in the Airflow webserver and scheduler pods).

. The DAGs are automatically loaded into Airflow, and the changes are reflected in the Airflow UI.


#### Steps to Set Up
Initialize Git Repository:

. Create a repository for storing your DAGs. For example:
```
git init airflow-dags-repo
cd airflow-dags-repo
mkdir dags

```
. Place your DAG files into the dags/ directory.

Configure Argo CD Application:

. Install Argo CD on your Kubernetes cluster following the official documentation.

. Create an Argo CD Application YAML file that points to the Git repository with your DAGs:
```
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: airflow-dags
  namespace: argocd
spec:
  project: default
  source:
    repoURL: 'https://github.com/your-username/airflow-dags-repo.git'
    path: dags
    targetRevision: HEAD
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: airflow
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

```

. Apply this file to Argo CD using:

```
kubectl apply -f airflow-application.yaml
```

Synchronize DAGs:

. When you push changes to the Git repository (new or updated DAGs), Argo CD will automatically pull those changes and deploy them to the Airflow cluster.

. You can monitor the sync status in the Argo CD UI or CLI:

```
argocd app get airflow-dags
```

Deploy New DAGs:

. To deploy a new DAG, simply add the Python file to the dags/ directory in your Git repository:

```
cd airflow-dags-repo/dags
touch new_dag.py
git add new_dag.py
git commit -m "Added new DAG"
git push origin main

```

#### Argo CD will detect the new file and deploy it to the Airflow cluster.

Advantages of Using GitOps with Argo CD

. Declarative Deployments: The state of the Airflow cluster (including DAGs) is described declaratively in Git, providing a single source of truth.

. Easy Rollbacks: Since all changes are version-controlled in Git, it’s straightforward to roll back to a previous state if a DAG introduces issues.

. Automated Sync: Argo CD handles the deployment of DAGs automatically, reducing manual intervention.

. Observability: Argo CD provides a UI to monitor which DAGs are synced and their current state, ensuring transparency in the deployment process.



#### Summary of Comparison:
#### GitOps with Argo CD:

#### Pros:

. Excellent for large-scale, multi-environment deployments.

. Full automation with robust version control, rollback, and observability.

. Kubernetes-native with full GitOps benefits (declarative state, audit trail, etc.).

. Ideal for teams practicing DevOps or GitOps methodologies with advanced infrastructure needs.

#### Cons:

. Higher complexity and setup overhead.

. Requires additional infrastructure (Argo CD installation and maintenance).

#### Git-Sync:

#### Pros:

. Lightweight and simple to set up, making it easy to integrate into existing Airflow Kubernetes setups.

. Works well for smaller environments and simpler workflows.

. Minimal infrastructure overhead – no need for additional tools beyond Git and Kubernetes.

#### Cons:

. Limited in terms of observability and rollback capabilities.
Syncs on a time interval (e.g., every 60 seconds) rather than immediately on changes.

. May not scale as easily for larger teams or more complex workflows.


#### Which One to Choose?

#### GitOps with Argo CD is better if:

. You have a complex Airflow setup (multiple clusters, environments).

. You need strong version control, rollback capabilities, and observability.

. You want to fully embrace GitOps or DevOps practices.

#### Git-Sync is better if:

. You’re looking for a simple, no-fuss method to sync DAGs.

. You don’t need advanced features like rollback or UI-based observability.

. You want minimal setup and infrastructure overhead.



### Deploying the Solution

Clone the repository

```
 git clone https://github.com/armevo4/opsfleet-task.git


```

Navigate into self-managed-airflow directory and run install.sh script

```
cd airplow-task/self-managed-airflow
chmod +x install.sh
./install.sh
```


### Verify the resources
#### Create kubectl config
Update the placeholder for AWS region and run the below command.

```
mv ~/.kube/config ~/.kube/config.bk
aws eks update-kubeconfig --region <region>  --name self-managed-airflow
```

### Describe the EKS Cluster

```
aws eks describe-cluster --name self-managed-airflow


```

### Verify the EFS PV and PVC created by this deployment

```
kubectl get pvc -n airflow

NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
airflow-dags   Bound    pvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            efs-sc         73m

kubectl get pv -n airflow
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE
pvc-157cc724-06d7-4171-a14d-something   10Gi       RWX            Delete           Bound    airflow/airflow-dags           efs-sc                  74m

```

### Verify the EFS Filesystem

```
aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text
```

### Verify S3 bucket created for Airflow logs

```
aws s3 ls | grep airflow-logs-


```

### Verify the Airflow deployment

```
kubectl get deployment -n airflow

NAME                READY   UP-TO-DATE   AVAILABLE   AGE
airflow-pgbouncer   1/1     1            1           77m
airflow-scheduler   2/2     2            2           77m
airflow-statsd      1/1     1            1           77m
airflow-triggerer   1/1     1            1           77m
airflow-webserver   2/2     2            2           77m

```

### Fetch Postgres RDS password
Amazon Postgres RDS database password can be fetched from the Secrets manager

. Login to AWS console and open secrets manager
. Click on postgres secret name
. Click on Retrieve secret value button to verify the Postgres DB master password

### Login to Airflow Web UI
This deployment creates an Ingress object with public LoadBalancer(internal # Private Load Balancer can only be accessed within the VPC) for demo purpose For production workloads, you can modify airflow-values.yaml to choose internal LB. In addition, it's also recommended to use Route53 for Airflow domain and ACM for generating certificates to access Airflow on HTTPS port.

Execute the following command to get the ALB DNS name

```
kubectl get ingress -n airflow

NAME                      CLASS   HOSTS   ADDRESS                                                                PORTS   AGE
airflow-airflow-ingress   alb     *       k8s-dataengineering-c92bfeb177-randomnumber.us-west-2.elb.amazonaws.com   80      88m

```

The above ALB URL will be different for you deployment. So use your URL and open it in a browser

By default, Airflow creates a default user with admin and password as admin

Login with Admin user and password and create new users for Admin and Viewer roles and delete the default admin user

### Execute Sample Airflow Job
. Login to Airflow WebUI

. Click on DAGs link on the top of the page. This will show dags pre-created by the GitSync feature

. Execute the hello_world_scheduled_dag DAG by clicking on Play button (>)

. Verify the DAG execution from Graph link

. All the Tasks will go green after few minutes

. Click on one of the green Task which opens a popup with log link where you can verify the logs pointing to S3



### How to Set Up Git-Sync:
#### 1. Create a Git Repository for DAGs

. Begin by creating a dedicated Git repository that will contain your DAG files. This repository will act as the source of truth for your DAGs.

. Initialize a new Git repository (if one doesn’t already exist) and add your DAGs to the repository:

```
git init
git add dags/  # assuming you have a dags/ directory
git commit -m "Initial commit of DAGs"
git remote add origin <your-git-repo-url>
git push origin master

```

#### 2. Configure Git-Sync in Airflow

. Modify the Airflow deployment to include a Git-Sync sidecar container in the airflow-webserver and airflow-scheduler pods. This container will clone and keep the DAGs synchronized from the Git repository to the local file system where Airflow can access them.

. Update the Airflow Helm values file or Kubernetes manifests to include the following Git-Sync configuration.

#### 3. Create Kubernetes Secret for Git Credentials

. To securely provide Git credentials to Git-Sync, create a Kubernetes secret. This secret will store your Git username and password/token used for authentication.

#### 4. Mount DAGs into Airflow

. Ensure that the DAGs synchronized by Git-Sync are accessible by the Airflow webserver and scheduler. You need to mount the same volume in the Airflow containers where the DAGs will be stored. In your Airflow deployment file, update the volume mounts.

#### 5. Test DAG Synchronization

. Once the Git-Sync sidecar is up and running, any changes you push to the Git repository should automatically appear in the /opt/airflow/dags directory within your Airflow pods.

. You can verify that the DAGs are synchronized by checking the contents of the dags directory inside the Airflow container.

```
kubectl exec -n airflow <airflow-webserver-pod> -- ls /opt/airflow/dags

```

. To test the workflow:

    . Push a new DAG to the Git repository.

    . Wait for Git-Sync to pull the new changes (within the configured polling interval).

    . Confirm the new DAG is listed in the Airflow UI.

#### 6. Monitor DAG Updates

. Git-Sync will continuously check for changes in the repository (based on the polling interval). Whenever a new commit is pushed to the main branch of your DAG repository, Git-Sync will fetch the updates, and Airflow will automatically detect the new DAGs.


. You can also monitor the Git-Sync logs to ensure synchronization is working correctly:

```
kubectl logs <airflow-webserver-pod> -c git-sync

```

